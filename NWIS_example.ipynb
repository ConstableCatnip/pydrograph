{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Parsing an NWIS dataset in Pandas\n",
    "NWIS offers web services that make it convenient to pull down water data for streamflow, wells, and other measurements. \n",
    "\n",
    "A full listing of available data and formats can be found from: http://waterdata.usgs.gov/nwis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# urllib is a package for interacting with websites\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We will need to process the daily value information at a site to get $Q_{90}$\n",
    "\n",
    "The site of interest is the Plover River in Wisconsin: 05400513\n",
    "\n",
    "The URL for pulling daily values for a period of record is:\n",
    "\n",
    "http://waterdata.usgs.gov/nwis/dv?cb_00060=on&format=rdb&site_no=05400513&referred_module=sw&period=&begin_date=2010-01-22&end_date=2015-01-22\n",
    "\n",
    "###First let's use urllib to download the entire file\n",
    "\n",
    "This has the advantage of letting us look at the file as a text file and parse some of the goofy NWIS formatting.\n",
    "\n",
    "Note that we will open the URL and read it down to a text file.\n",
    "\n",
    "For more information on how to construct these URLs, see:\n",
    "http://waterservices.usgs.gov/rest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dv_URL = \"http://waterdata.usgs.gov/nwis/dv?cb_00060=on&format=rdb&site_no=05400513&referred_module=sw&period=&begin_date=2010-01-22&end_date=2015-01-22\"\n",
    "dv_file = urllib.URLopener()\n",
    "dv_file.retrieve(dv_URL, \"DV_Plover_05400513.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's read this in and check out the first 40 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "NWISfilename = os.path.join(\"..\",\"Data\",\"12_pandas data\",\n",
    "                                \"DV_Plover_05400513.dat\")\n",
    "reconnoiter = open(NWISfilename, 'r').readlines()\n",
    "for i in np.arange(40):\n",
    "    print reconnoiter[i].rstrip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Test your skillz\n",
    "How many lines start with '#'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numhash = 0 #let's use the as the counter\n",
    "for line in reconnoiter:\n",
    "    if line.startswith('#'):\n",
    "        numhash +=1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print numhash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###To automate reading the file\n",
    "What is the main challenge here? We can easily tell Pandas that '#' means \"comment\", but then we have to interpret the column names. \n",
    "\n",
    "One way, if we know there are always going to be two header-type lines after the last '#' line, we can just add 2 to numhash, skip that many rows, and read directly from the URL using Pandas `read_csv` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colnames = ['agency','station','date','flow','flag']\n",
    "\n",
    "nwis_df_url = pd.read_csv(dv_URL,\n",
    "                          sep = '\\s+',\n",
    "                          skiprows = numhash+2,\n",
    "                          names = colnames,\n",
    "                          parse_dates = 2,\n",
    "                          index_col = 2)\n",
    "\n",
    "# NOTE We can also read in the text file we downloaded instead\n",
    "nwis_df_txt = pd.read_csv(NWISfilename,\n",
    "                          sep = '\\s+',\n",
    "                          skiprows = numhash+2,\n",
    "                          names = colnames,\n",
    "                          parse_dates = 2,\n",
    "                          index_col = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###How does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nwis_df_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, but we have two non-numeric values. \"Dis\" and \"NaN\". \n",
    "\n",
    "There's a great method called `dropna` we can use to just remove all rows that have `NaN` values.\n",
    "http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.dropna.html\n",
    "\n",
    "How can we make all the nonnumeric values into \"NaN\"?\n",
    "http://stackoverflow.com/questions/17097236/how-to-replace-values-with-none-in-pandas-data-frame-in-python\n",
    "\n",
    "NOTE: Both methods have the optional Boolean `inplace` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# so, first replace \"Dis\" with \"NaN\"\n",
    "nwis_df_url.replace('Dis', np.nan, inplace = True)\n",
    "print '{0} rows in nwis_df_url'.format(len(nwis_df_url))\n",
    "nwis_df_url.dropna(inplace=True)\n",
    "print '{0} rows in nwis_df_url'.format(len(nwis_df_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Number conversions\n",
    "Because we had a mixture of text and numbers, we need to convert flow to float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at the data types for each column in the DataFrame\n",
    "nwis_df_url.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert flow to float (NOTE...when no spaces are in a column name, can use '.')\n",
    "nwis_df_url.flow=nwis_df_url.flow.astype(float)\n",
    "\n",
    "#or equivalently\n",
    "nwis_df_url['flow']=nwis_df_url['flow'].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We can plot with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(nwis_df_url.index,nwis_df_url['flow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Now we can plot the flow over the entire time series with Pandas\n",
    "Note how much nicer the dates look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nwis_df_url['flow'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Let's look at things at various time periods\n",
    "First, we can make a couple new columns, one for year, and one for water year.\n",
    "\n",
    "How can we group by water year? Not a very easy Google Kung Fu exercise at first, but what about \"Fiscal Year\"?\n",
    "Google \"Pandas group by fiscal year\"\n",
    "http://stackoverflow.com/questions/26341272/using-groupby-on-pandas-dataframe-to-group-by-financial-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a year column from the date index\n",
    "nwis_df_url[u'year'] = nwis_df_url.index.year\n",
    "#make water year by shifting forward the number of days in Oct., Nov., and Dec.\n",
    "# NOTE --> shifting by months is less precise\n",
    "nwis_df_url[u'wateryear'] = nwis_df_url.index.shift(30+31+31,freq='d').year\n",
    "nwis_df_url[['year','wateryear']].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Now we can use the groupby method \n",
    "Groupby is super powerful and can be used to learn metrics on a year to year basis in this case. We could also, of course, use this to group by other characteristics such as wellname if we had multiple wells, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Annual Mean'\n",
    "print nwis_df_url.groupby('year')['flow'].mean()\n",
    "print '\\n\\nWater Year Mean'\n",
    "print nwis_df_url.groupby('wateryear')['flow'].mean()\n",
    "print '\\n\\nWater Year 10th percentile'\n",
    "print nwis_df_url.groupby('wateryear')['flow'].quantile(0.1)\n",
    "print '\\n\\nWater Year 90th percentile'\n",
    "print nwis_df_url.groupby('wateryear')['flow'].quantile(0.9)\n",
    "print '\\n\\nWater Year '\n",
    "print nwis_df_url.groupby('wateryear')['flow'].max()\n",
    "print '\\n\\n10th Percentile over entire record'\n",
    "print nwis_df_url['flow'].quantile(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We can plot these as bar charts as well, if we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nwis_df_url.groupby('year')['flow'].quantile(0.1).plot(kind='bar', title='year Q90')\n",
    "plt.figure()\n",
    "nwis_df_url.groupby('wateryear')['flow'].quantile(0.1).plot(kind='bar', title='wateryear Q90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
